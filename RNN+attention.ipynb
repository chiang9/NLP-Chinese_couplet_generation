{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817dcb9-150c-4739-8906-c4ea6a7c0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130ff9c-052e-46c6-8ea2-a4778a3a0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用的paddlenlp的word embeddings，具体看 https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/model_zoo/embeddings.md\n",
    "# data封装类型还没弄懂 把batch封装好了就能train了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a579f1e-6eb3-403d-a3d6-a47803294e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import timeit\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18458aa6-5e29-461a-bcaa-00e8611bb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_in = open(\"couplet/train/in.txt\",encoding='utf8').read()\n",
    "tr_out = open(\"couplet/train/out.txt\",encoding='utf8').read()\n",
    "te_in = open(\"couplet/test/in.txt\",encoding='utf8').read()\n",
    "te_out = open(\"couplet/test/out.txt\",encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a356c600-cbaa-4db9-8810-28ece4d4643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(zip(te_in.split('\\n')[:-1], te_out.split('\\n')[:-1]))\n",
    "dev =  list(zip(tr_in.split('\\n')[-3000:], tr_out.split('\\n')[-3000:]))\n",
    "train = list(zip(tr_in.split('\\n')[:-3000], tr_out.split('\\n')[:-3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31c1d3d-0a72-4fef-bccd-33713c9379dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770492, 767492, 3000, 4000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_in.split('\\n')),len(train),len(dev),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6e28bf-1853-4009-b542-0da7fd89a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-28 21:58:29,527] [    INFO] - Loading token embedding...\n",
      "[2021-11-28 21:58:40,332] [    INFO] - Finish loading embedding vector.\n",
      "[2021-11-28 21:58:40,333] [    INFO] - Token Embedding info:             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Shape :[635965, 300]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82048191-7104-40b1-a6ea-3ef78ff8b312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('晚',\n",
       " array([[ 4.10297e-01,  2.66597e-01,  2.08996e-01, -2.67891e-01,\n",
       "          2.28325e-01,  5.52471e-01,  4.78919e-01, -1.79455e-01,\n",
       "          3.38603e-01, -3.12654e-01, -1.47581e-01, -1.69136e-01,\n",
       "         -1.98575e-01,  2.36687e-01, -3.78675e-01,  2.80865e-01,\n",
       "          2.33982e-01, -3.65770e-02,  1.09713e-01,  3.70740e-02,\n",
       "          3.87628e-01, -2.69209e-01,  4.85920e-02,  3.79245e-01,\n",
       "         -7.54320e-02,  6.61160e-02,  2.95900e-01, -5.35660e-02,\n",
       "         -2.86690e-01,  2.93964e-01,  2.39861e-01, -2.35460e-02,\n",
       "          2.07960e-02,  1.27202e-01,  3.09681e-01,  2.66898e-01,\n",
       "         -3.26066e-01, -2.18838e-01,  1.18878e-01, -3.63457e-01,\n",
       "          2.82700e-03,  3.07751e-01, -2.07518e-01, -3.25977e-01,\n",
       "         -1.46433e-01,  2.18737e-01, -3.79057e-01,  6.31100e-02,\n",
       "         -2.72110e-01,  6.88830e-02,  8.38940e-02, -1.57267e-01,\n",
       "         -2.13026e-01,  1.27516e-01, -2.30970e-01,  7.06990e-02,\n",
       "         -7.58360e-02,  6.21128e-01,  2.34140e-01,  2.42040e-01,\n",
       "          2.66138e-01,  2.61553e-01,  5.04840e-02,  2.15590e-02,\n",
       "          4.56000e-02, -1.13656e-01,  8.07790e-02,  2.12347e-01,\n",
       "          4.60490e-02, -1.72840e-02, -4.11008e-01,  2.35820e-02,\n",
       "          1.12237e-01,  3.98300e-01,  2.67884e-01, -7.49330e-02,\n",
       "          3.19010e-01, -2.17765e-01,  1.18287e-01, -2.28215e-01,\n",
       "          5.38300e-03,  2.30568e-01, -1.07300e-02, -4.17410e-01,\n",
       "          1.61930e-02, -1.14668e-01,  2.32117e-01,  2.79430e-01,\n",
       "          2.00785e-01, -6.63210e-02,  4.27250e-01,  4.52675e-01,\n",
       "          2.47658e-01, -5.86300e-02,  4.39808e-01, -3.70100e-03,\n",
       "         -5.23020e-01, -1.07206e-01, -1.92198e-01, -2.55553e-01,\n",
       "          6.45600e-03, -7.57860e-02,  2.83562e-01,  2.73646e-01,\n",
       "          4.30020e-01, -3.43621e-01,  6.69510e-02, -1.87965e-01,\n",
       "          2.28966e-01,  7.16910e-02,  1.14580e-01, -5.11165e-01,\n",
       "          5.60186e-01, -2.90680e-02,  1.00238e-01, -3.12676e-01,\n",
       "          2.55616e-01,  5.89700e-02, -1.22141e-01, -3.66039e-01,\n",
       "          1.41547e-01,  3.79173e-01, -2.52095e-01,  8.44600e-03,\n",
       "         -9.74950e-02, -3.90637e-01, -1.49284e-01,  7.28700e-03,\n",
       "          1.40770e-01,  4.61778e-01,  1.10123e-01, -3.52360e-02,\n",
       "         -1.31000e-02,  1.52499e-01,  6.13391e-01,  4.55530e-01,\n",
       "          3.04193e-01,  1.94345e-01,  1.11100e-03,  4.56726e-01,\n",
       "          2.48794e-01, -2.38452e-01, -1.25242e-01, -1.76355e-01,\n",
       "          4.09390e-02,  5.09543e-01,  3.89089e-01, -2.74697e-01,\n",
       "          4.49087e-01,  4.17064e-01, -1.25365e-01, -3.98724e-01,\n",
       "          2.28932e-01,  2.10964e-01,  1.30018e-01,  1.53666e-01,\n",
       "          2.22167e-01, -2.93432e-01, -2.49894e-01, -2.28974e-01,\n",
       "         -5.54150e-02, -2.67940e-02, -1.95460e-02, -4.68514e-01,\n",
       "         -4.08100e-01,  8.05856e-01,  2.08010e-02,  2.84860e-02,\n",
       "         -1.77957e-01,  4.51340e-02,  2.36590e-02,  2.70059e-01,\n",
       "         -1.96600e-03,  1.12154e-01, -1.42744e-01,  2.08223e-01,\n",
       "         -1.55980e-01, -2.39023e-01, -3.14068e-01,  1.87740e-02,\n",
       "          2.42906e-01,  1.38118e-01, -1.17372e-01, -7.71580e-02,\n",
       "          1.84067e-01,  1.59011e-01, -6.46260e-02, -6.00330e-01,\n",
       "         -6.87319e-01,  4.33118e-01,  1.64402e-01, -4.03055e-01,\n",
       "          1.58385e-01,  3.01439e-01, -3.05615e-01, -9.99030e-02,\n",
       "         -1.14280e-02,  5.29260e-02, -2.83557e-01,  4.57242e-01,\n",
       "         -4.87730e-02, -1.04710e-01,  8.25740e-02,  2.90432e-01,\n",
       "          3.87940e-02, -3.87936e-01,  3.67940e-01,  9.67790e-02,\n",
       "          2.88980e-01, -4.48325e-01,  2.92663e-01, -1.47756e-01,\n",
       "         -2.25867e-01,  2.61304e-01, -1.39689e-01, -2.51330e-01,\n",
       "         -2.17429e-01,  6.30320e-02,  2.30607e-01, -6.63300e-03,\n",
       "          7.34620e-02, -1.88228e-01,  1.43349e-01, -1.01166e-01,\n",
       "         -2.98960e-02, -7.28150e-02,  1.49355e-01, -3.25654e-01,\n",
       "          1.31150e-02, -2.14721e-01, -5.69200e-02,  3.36038e-01,\n",
       "         -1.08493e-01, -9.74280e-02,  2.04043e-01,  1.86769e-01,\n",
       "          3.43347e-01,  4.67750e-02,  2.32867e-01,  1.16131e-01,\n",
       "          8.75400e-02,  2.31193e-01, -4.96168e-01,  8.78550e-02,\n",
       "         -3.57289e-01, -3.05284e-01, -1.90271e-01,  4.21240e-01,\n",
       "          2.53314e-01, -1.17420e-01, -3.62615e-01, -4.54940e-01,\n",
       "         -2.68576e-01, -5.69708e-01, -1.23790e-02,  8.78700e-02,\n",
       "         -3.01957e-01, -1.84959e-01, -8.98230e-02, -1.42554e-01,\n",
       "          7.59303e-01, -4.57990e-02, -3.41431e-01, -2.31250e-01,\n",
       "          3.71190e-02,  2.95303e-01,  4.27752e-01,  5.57590e-02,\n",
       "          2.20661e-01,  4.21783e-01, -5.20091e-01, -1.26292e-01,\n",
       "         -1.35566e-01, -2.21661e-01,  7.28000e-04,  3.97608e-01,\n",
       "          3.97467e-01,  9.12150e-02, -1.14176e-01,  7.15678e-01,\n",
       "          2.53680e-02,  9.00520e-02, -4.63188e-01, -4.88830e-02,\n",
       "          2.18726e-01,  7.90360e-02, -1.36008e-01, -1.10540e-01,\n",
       "         -1.21530e-02, -3.79370e-02, -5.22900e-03,  1.00693e-01,\n",
       "          1.70331e-01,  2.41695e-01,  8.96960e-02,  5.58100e-03,\n",
       "          4.06972e-01, -5.83440e-02, -3.08000e-02,  1.48932e-01]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'晚', token_embedding.search('晚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6b63c9-b912-4075-93ba-7948fc49c1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9132, 9132)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(tr_in + tr_out + te_in + te_out))\n",
    "\n",
    "embeddings = dict()\n",
    "for w in vocab:\n",
    "    embeddings[w] = token_embedding.search(w).reshape(300,)\n",
    "\n",
    "embeddings['<s>'] = np.random.rand(300,).astype('float32') *2 - 1 # range(-1:1)\n",
    "embeddings['<e>'] = np.random.rand(300,).astype('float32') *2 - 1 # range(-1:1)\n",
    "embeddings['<pad>'] = np.zeros(300,)\n",
    "vocab.insert(0,'<e>')\n",
    "vocab.insert(0,'<s>')\n",
    "vocab.insert(0,'<pad>')\n",
    "# creating word2id lookup dictionary\n",
    "word2id = {w:i for i,w in enumerate(embeddings,start=3)}\n",
    "word2id['<pad>'] = 0\n",
    "word2id['<s>'] = 1\n",
    "word2id['<e>'] = 2 \n",
    "id2word = {i:w for w,i in word2id.items()}\n",
    "\n",
    "max_sts_len = 20\n",
    "\n",
    "len(vocab),len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eae7a95-7367-4010-9f19-d2ebf17b93af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<s>', '<e>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0],id2word[1],id2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef3ea12-3d16-4d33-b9cf-d442e683ce4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '晚', '风', '摇', '树', '树', '还', '挺', '<e>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = train[0][0]\n",
    "['<s>'] + sent.split() + ['<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f714f1-2e11-447d-ba2a-456b522c91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x(sent,embeddings,max_sts_len=20): ##上联\n",
    "\n",
    "    word_list = []\n",
    "    #review = upper.strip().split(' ')\n",
    "    review = ['<s>'] + sent.split() + ['<e>'] \n",
    "    \n",
    "    if len(review) >= max_sts_len:\n",
    "        review = review[:max_sts_len]\n",
    "        origanal_len = max_sts_len\n",
    "    else:\n",
    "        origanal_len = len(review)\n",
    "        for i in range(len(review), max_sts_len):\n",
    "            review.append('<pad>') ## 词向量维度为300\n",
    "            \n",
    "    for word in review:                        \n",
    "        embedding_vector = embeddings[word]\n",
    "        word_list.append(embedding_vector)   \n",
    "\n",
    "    #word_list.append([origanal_len for j in range(300)]) ## 最后一行元素为句子实际长度\n",
    "    word_list = np.stack(word_list) \n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365a1761-fc97-479b-8b5a-f9feb7e03d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_y(sent,word2ix,max_sts_len=20):\n",
    "\n",
    "    word_list = [1] ##开头加起始符号1\n",
    "    for word in sent.split():\n",
    "        word_idx = word2ix[word]\n",
    "        word_list.append(word_idx)\n",
    "    word_list.append(2) ##结束加终止符号2\n",
    "    \n",
    "    if len(word_list) >= max_sts_len:\n",
    "        origanal_len = max_sts_len\n",
    "        word_list = word_list[:max_sts_len]\n",
    "    else:\n",
    "        origanal_len = len(word_list)\n",
    "        for i in range(len(word_list), max_sts_len):\n",
    "            word_list.append(0) ## 不够长度则补0  \n",
    "    #word_list.append(origanal_len) ##最后一个元素为句子长度\n",
    "    \n",
    "    return np.array(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd7d026-6845-4e7b-9b9b-dd9f7b6f22a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 300), '晚 风 摇 树 树 还 挺 ')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[(i,len(k[0].split())) for i,k in enumerate(train) if len(k[0].split()) >= 20]\n",
    "sent_x,sent_y = train[0][0],train[0][1]#,train[12864][0]\n",
    "prepare_x(sent_x,embeddings).shape,sent_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e56148-eccc-4e1f-8c0d-b4b606976889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1, 4678,  115, 7833, 4887, 4887,  626, 1980,    2,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " '晨 露 润 花 花 更 红 ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_y(sent_y,word2id),sent_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0556a1-723e-4b70-a2e8-a5ec63894d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    " \n",
    "        self.embedding_dim = embedding_dim #词向量维度，本项目中是300维\n",
    "        self.hidden_dim = hidden_dim #RNN隐层维度\n",
    "        self.num_layers = num_layers #RNN层数\n",
    "        self.dropout = dropout  #dropout\n",
    " \n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim,\n",
    "                          num_layers=num_layers, dropout=dropout)\n",
    " \n",
    "        self.dropout = nn.Dropout(dropout) #dropout层\n",
    " \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # src = [sent len, batch size]\n",
    "        embedded = self.dropout(input_seqs)\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        #packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) #将输入转换成torch中的pack格式，使得RNN输入的是真实长度的句子而非padding后的\n",
    "        #outputs, hidden = self.rnn(packed, hidden)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # outputs, hidden = self.rnn(embedded, hidden)\n",
    "        # outputs = [sent len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # outputs are always from the last layer\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc1f6684-02c3-42a0-922f-3e4ba87372af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(self.hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "        self.v.data.normal_(mean=0, std=1. / np.sqrt(self.v.size(0)))\n",
    " \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #  encoder_outputs:(seq_len, batch_size, hidden_size)\n",
    "        #  hidden:(num_layers * num_directions, batch_size, hidden_size)\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        h = hidden[-1].repeat(max_len, 1, 1)\n",
    "        # (seq_len, batch_size, hidden_size)\n",
    "        attn_energies = self.score(h, encoder_outputs)  # compute attention score\n",
    "        return F.softmax(attn_energies, dim=1)  # normalize with softmax\n",
    " \n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # (seq_len, batch_size, 2*hidden_size)-> (seq_len, batch_size, hidden_size)\n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        energy = energy.permute(1, 2, 0)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.repeat(encoder_outputs.size(1), 1).unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        energy = torch.bmm(v, energy)  # (batch_size, 1, seq_len)\n",
    "        return energy.squeeze(1)  # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c02c73-0247-4605-b2cf-9db82418ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    " \n",
    "        self.embedding_dim = embedding_dim ##编码维度\n",
    "        self.hid_dim = hidden_dim ##RNN隐层单元数\n",
    "        self.output_dim = output_dim ##词袋大小\n",
    "        self.num_layers = num_layers ##RNN层数\n",
    "        self.dropout = dropout\n",
    " \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim,\n",
    "                          num_layers=num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(embedding_dim + hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [bsz]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # encoder_outputs = [sent len, batch size, hid dim * n directions]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, bsz]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, bsz, emb dim]\n",
    "        attn_weight = self.attention(hidden, encoder_outputs)\n",
    "        # (batch_size, seq_len)\n",
    "        context = attn_weight.unsqueeze(1).bmm(encoder_outputs.transpose(0, 1)).transpose(0, 1)\n",
    "        # (batch_size, 1, hidden_dim * n_directions)\n",
    "        # (1, batch_size, hidden_dim * n_directions)\n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "        # emb_con = [1, bsz, emb dim + hid dim]\n",
    "        _, hidden = self.rnn(emb_con, hidden)\n",
    "        # outputs = [sent len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        output = torch.cat((embedded.squeeze(0), hidden[-1], context.squeeze(0)), dim=1)\n",
    "        output = F.log_softmax(self.out(output), 1)\n",
    "        # outputs = [sent len, batch size, vocab_size]\n",
    "        return output, hidden, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8b50c53-cbdf-4b14-8d1d-72508a063c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    " \n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    " \n",
    "    def forward(self, src_seqs, src_lengths, trg_seqs):\n",
    "        # src_seqs = [sent len, batch size]\n",
    "        # trg_seqs = [sent len, batch size]\n",
    "        batch_size = src_seqs.shape[1]\n",
    "        max_len = trg_seqs.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # hidden used as the initial hidden state of the decoder\n",
    "        # encoder_outputs used to compute context\n",
    "        encoder_outputs, hidden = self.encoder(src_seqs, src_lengths)\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        output = trg_seqs[0, :]\n",
    " \n",
    "        for t in range(1, max_len): # skip sos\n",
    "            output, hidden, _ = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            output = (trg_seqs[t] if teacher_force else output.max(1)[1])\n",
    "        return outputs\n",
    " \n",
    "    def predict(self, src_seqs, src_lengths, max_trg_len=30, start_ix=1):\n",
    "        max_src_len = src_seqs.shape[0]\n",
    "        batch_size = src_seqs.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src_seqs, src_lengths)\n",
    "        output = torch.LongTensor([start_ix] * batch_size).to(self.device)\n",
    "        attn_weights = torch.zeros((max_trg_len, batch_size, max_src_len))\n",
    "        for t in range(1, max_trg_len):\n",
    "            output, hidden, attn_weight = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            output = output.max(1)[1]\n",
    "            #attn_weights[t] = attn_weight\n",
    "        return outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41998b5-ee75-4be1-9a9f-4007a9fd22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en=Encoder(300,256) ##词向量维度300，rnn隐单元256\n",
    "de=Decoder(len(word2id),300,256) ##词袋大小9132，词向量维度300，rnn隐单元256\n",
    "network = Net(en,de,device) ##定义Seq2Seq实例\n",
    "loss_fn = nn.CrossEntropyLoss() ##使用交叉熵损失函数\n",
    "optimizer = optim.Adam(network.parameters(),lr=0.01) ##使用Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8671616-3956-460f-8256-c992ae8f77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader, (batch data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d939b2-358e-4cb7-8879-fd2284754791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_loss = 10\n",
    "EPOCHS = 5\n",
    "# 得到训练和测试的数据\n",
    "for epoch in range(EPOCHS):\n",
    "    network.train()\n",
    "    \n",
    "    # 得到训练和测试的数据\n",
    "    x_train, y_train, x_test, y_test = data.next_batch(BATCH)  # 读取数据; shape:(sen_len,batch,embedding)\n",
    "    #x_train shape: (batch,sen_len,embed_dim)\n",
    "    #y_train shape: (batch,sen_len)\n",
    "    batch_len = y_train.shape[0]\n",
    "    #input_lengths = [30 for i in range(batch_len)] ## batch内每个句子的长度\n",
    "    input_lengths = x_train[:,-1,0]\n",
    "    input_lengths = input_lengths.tolist()\n",
    "    #input_lengths = list(map(lambda x: int(x),input_lengths))\n",
    "    input_lengths = [int(x) for x in input_lengths]\n",
    "    y_lengths = y_train[:,-1]\n",
    "    y_lengths = y_lengths.tolist()\n",
    "    \n",
    "    x_train = x_train[:,:-1,:] ## 除去长度信息\n",
    "    x_train = torch.from_numpy(x_train) #shape:(batch,sen_len,embedding)\n",
    "    x_train = x_train.float().to(device) \n",
    "    y_train = y_train[:,:-1] ## 除去长度信息\n",
    "    y_train = torch.from_numpy(y_train) #shape:(batch,sen_len)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.to(device) \n",
    " \n",
    "    seq_pairs = sorted(zip(x_train.contiguous(), y_train.contiguous(),input_lengths), key=lambda x: x[2], reverse=True)\n",
    "    #input_lengths = sorted(input_lengths, key=lambda x: input_lengths, reverse=True)\n",
    "    x_train, y_train,input_lengths = zip(*seq_pairs)\n",
    "    x_train = torch.stack(x_train,dim=0).permute(1,0,2).contiguous()\n",
    "    y_train = torch.stack(y_train,dim=0).permute(1,0).contiguous()\n",
    " \n",
    "    outputs = network(x_train,input_lengths,y_train)\n",
    "    \n",
    "    #_, prediction = torch.max(outputs.data, 2)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    outputs = outputs.float()\n",
    "    # calculate the loss according to labels\n",
    "    loss = loss_fn(outputs.view(-1, outputs.shape[2]), y_train.view(-1))\n",
    " \n",
    "    # backward transmit loss\n",
    "    loss.backward()\n",
    "    # adjust parameters using Adam\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    " \n",
    "    # 若测试准确率高于当前最高准确率，则保存模型\n",
    " \n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        model.save_model(network, MODEL_PATH, overwrite=True)\n",
    "        print(\"step %d, best lowest_loss %g\" % (epoch, lowest_loss))\n",
    "    print(str(epoch) + \"/\" + str(args.EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5c48d-46eb-45b4-bed8-a318d127b4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
