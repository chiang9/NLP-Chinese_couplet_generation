%
% File acl2015.tex
%
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Chinese Couplet Generation Proposal}

\author{
  Kuan-Yu Chiang\\
  \texttt{kuanyu@usc.edu}
  \\\And
  Qian Yin\\
  \texttt{qianyin@usc.edu}
  \\\And
  Joe Chen\\
  \texttt{zchen462@usc.edu}
  \\\AND
  Shihao Lin\\
  \texttt{shihaol@usc.edu}
  \\\And
  Qizhen Jin\\
  \texttt{qizhenji@usc.edu}
}
 
\date{2021/10/19}

\begin{document}
\maketitle

\section{Project Domain and Goals}

Poetry is a form of literature that is arranged for its meaning and rhythm, and it has been proven that it plays a significant role in language development and the advancement of history. Among the diversity of literature, Chinese couplet stands out with a special poetry form using pairs of orderly sentences. With the nurture of over 1000 years, the Chinese couplet has become a unique art and historical treasure, however, due to the complexity of semantic and grammatical rules, the creation of a suitable couplet is a formidable challenge.

The complexity of the semantic and grammatical rules of Chinese couplets tends to address greater difficulty to the data preprocessing and model design. More attention is needed in processing the Chinese language data. In this paper, we will formulate the language generation problem by utilizing the Nature Language Processing (NLP) techniques and neural networks. Natural Language Processing will be helpful in preprocessing the data, and the neural network structure model is effective in solving the text generation problem. With this application, we will be able to successfully generate the Chinese couplets, and we aim to extend the model to apply and generate other types of literature.

Our objective in this project is to improve the previous NLP work on the Chinese Couplet’s generation. We aim to implement an attention-based Transformer model for the couplet generation. Different from the previous research, we will focus on designing a model that specifically fits the linguistic features of Chinese couplets. This includes adapting the model to meet the restricted requirements of pattern, vocabulary, and flatness for Chinese Couplets.


\section{Related Work}

Research in text generation first appeared in the 1970s ~\cite{Goldman:74}. And in the last decade, with the availability of large collections of datasets, automated conditional text generation rose to popularity. There have been several different approaches to the generation of Chinese poetry that have had success. 

Each couplet is made up of two sentences, and given the first sentence, one can use Statistical Machine Translation model to generate the second sentence ~\cite{Zhou:09}. Because of the special structure of Chinese couplets, each character in the first sentence have a strong connection to the character in the same position in the second sentence. Therefore, Zhou, M. et al.~\shortcite{Zhou:09} used a phrase-based SMT to “translate” each character of the first sentence to a list of candidate characters that could act as its counterpart in the second sentence. Then, filter out candidates that do not satisfy linguistic constraints of couplets. 

Xingxing Z. et al.~\shortcite{Zhang:14} drew inspiration from sequence to sequence learning using neural networks. They proposed a Recurrent Neural Networks model that generated quatrains based on some given keywords. The RNN learns the representation of different characters and the interaction between characters through a large collection of poems and generates lines in a poem based on the previous lines probabilistically. In contrast to the previous study by Zhou, M. et al ~\shortcite{Zhou:09}, Xingxing Z. et al.~\shortcite{Zhang:14} made no Markov assumptions about the dependency of the characters within a line. 

In recent years, the development of an attention mechanism based Encoder-Decoder framework in the NLP area has made it possible for computers to understand the text better. Google researchers Ashish V. et al. ~\shortcite{Vaswani:17} proposed a new simple network architecture called the Transformer. Although the Transformer is solely based on the attention mechanism, it perfectly preserved the recurrence and convolutions in previous neural networks models. 

J. Zhang, et al ~\shortcite{Zhang:18} introduced an open-source Chinese couplets generation system called VV-Couplets. It is an attention-based sequence to sequence neural model that maps the first line of couplets to the second line, generating similar output text giving the input. Compared to previous work on Chinese couplets, they addressed the entity names of person and location specially in their model.

Although the Transformer model is widely used in the Natural Language Processing area, the existent framework is not fully explored for linguistic characteristics in Chinese. The vocabularies of Chinese couplets are anticipated and obsolete. Therefore, Wang Y, Zhang J, Zhang B, Jin Q ~\shortcite{Wang:21} further improve the Transformer’s performance on Chinese couplets by intentionally adding POS taggings for special patterns in couplets, unregistered ancient words, and a polishing mechanism to improve the coherence.

\section{Datasets}

To make the results of our model more convincing, we perform experiments on the \textbf{Chinese Couplets Dataset} and \textbf{Chinese Poetry Dataset}.

\subsection{Chinese Couplets Dataset}

Chinese Couplets Dataset is available on \url{https://github.com/v-zich/couplet-clean-dataset}, which comes from an existing dataset in GitHub and contains around 740,000 couplets. Sensitive words in this dataset are deleted by searching the existing sensitive word vocabulary.

\subsection{Chinese Poetry Dataset}

Chinese Poetry Dataset is available on \url{https://github.com/hlthu/Chinese-Poetry-Dataset}, which comes from an existing dataset in GitHub and contains about 55,000 Tang poems. From a grammatical point of view, the third and fourth sentences basically conform to the grammar of couplets in Tang poetry. Therefore, these sentences can be used as training couplets. The number of our datasets will increase by around 40,000.

\subsection{Data Preprocessing}

Word embedding is a standard method for processing text and sequences. Word embedding solves the shortcomings of one-hot encoding-high dimensionality and irrelevance~\cite{Mikolov:13}. Moreover, the spatial distance of the vector can reflect the semantic relevance between words. Generally speaking, embedding is a dictionary that maps integer indices to dense vectors. It receives integers as input, looks up these integers in the dictionary, and returns the associated vector. In our model, we use word embedding to vectorize all words for preprocessing.


\section{Technical Challenge}

Developing a model for the Chinese Couplet generation does not have a closed form solution. Meanwhile, writing a well matched Chinese Couplet for a given sentence is a challenging task for humans. In short, Chinese Couplet generation can be considered as a sequence-to-sequence question. Many researchers have tried to conquer this problem with LSTM, RNN, and transformer related models. In this project, we aim to improve the existing algorithms from two directions: embedding the Chinese Tokenization and a new decording mechanism.



\subsection{Chinese Tokenization}

In Chinese, each character can be considered as a word. But most of the words in the vocabulary consist of more than one character. There are more than three thousand common characters in Chinese. Hence, Chinese Tokenization itself is a challenging problem in the Chinese-NLP domain. Most of the existing research on Chinese Couplet generation does not embed Chinese Word tokenization step into their models. Only consider single character embedding, the model will inevitably lose some Information related to the connection between characters. Therefore, we want to test whether the embedment of the Chinese tokenization can help to fill up the information gap.

\subsection{Decoding Mechanism}
\label{sect:pdf}

With the inspiration of the polish-up mechanism and bidirectional encoder model, we aim to develop a bidirectional decoder structure, which has not been seen on the existing works. In a regular sequence-to-sequence model, the output is generated one by one from the beginning of the sentence and provides the information for the later prediction.


\subsection{Evaluation Metrics}
\label{ssec:layout}

To evaluate the quality of the generated couplets, we would mainly use three well-known automatic machine evaluation metrics: Preplexity score, Bilingual Evaluation Understudy (BLEU), and Rouge Score. For language models, preplexity is the most common use metric. A lower preplexity score indicates the model can generate a fluent couplet. A high BLEU or Rough score shows the model is capable of generating a couplet that is similar to the gold standard. 


\begin{thebibliography}{}

\bibitem[\protect\citename{Goldman}1974]{Goldman:74}
Neil M. Goldman
\newblock 1974.
\newblock {\em Computer Generation of Natural Language from a Deep Conceptual Base}.
\newblock Ph.D. thesis, Standford University
\newblock Stanford AI Laboratory Memo AIM-247 or CS Technical Report CS-74-461

\bibitem[\protect\citename{Zhou \bgroup et al.\egroup }2009]{Zhou:09}
Zhou, M., Jiang, L.,and He, J.
\newblock 2009.
\newblock {\em Generating Chinese couplets and quatrain using a statistical approach}
\newblock Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Vol. 1 (pp. 43-52).

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013]{Mikolov:13}
Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey
\newblock 2013.
\newblock {\em Distributed Representations of Words and Phrases and their Compositionality}
\newblock Advances in Neural Information Processing Systems, Vol. 26, 2013

\bibitem[\protect\citename{Zhang \bgroup et al.\egroup }2014]{Zhang:14}
Zhang, Xingxing, and Mirella Lapata.
\newblock 2014.
\newblock {\em Chinese poetry generation with recurrent neural networks.}
\newblock Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014

\bibitem[\protect\citename{Zhang \bgroup et al.\egroup }2014]{Zhang:14}
Zhang, Xingxing, and Mirella Lapata.
\newblock 2014.
\newblock {\em Chinese poetry generation with recurrent neural networks.}
\newblock Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014

\bibitem[\protect\citename{Vaswani \bgroup et al.\egroup }2017]{Vaswani:17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
\newblock 2017.
\newblock {\em Attention is All you Need.}
\newblock NIPS 2017: 5998-6008.

\bibitem[\protect\citename{Zhang \bgroup et al.\egroup }2018]{Zhang:18}
Jiyuan Zhang, Zheling Zhang, Shiyue Zhang and Dong Wang
\newblock 2018.
\newblock {\em VV-Couplet: An open source Chinese couplet
generation system}
\newblock Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018, pp. 1756-1760, doi: 10.23919/APSIPA.2018.8659474. 

% \bibitem[\protect\citename{Sutskever \bgroup et al.\egroup }2020]{Ilya:20}
% Ilya Sutskever, Oriol Vinyals, and Quoc V. Le
% \newblock 2020.
% \newblock {\em Sequence to Sequence Learning with Neural Networks}
% \newblock Advances in Neural Information Processing Systems
% \newblock Curran Associates, Inc.

% \bibitem[\protect\citename{Yuan \bgroup et al.\egroup }2020]{Yuan:20}
% Shengqiong Yuan, Luo Zhong and Lin Li
% \newblock 2020.
% \newblock {\em Image inspired Chinese couplet generation}
% \newblock Web Intelligence 18, (2020), 217–227

\bibitem[\protect\citename{Wang \bgroup et al.\egroup }2021]{Wang:21}
Yufeng Wang, Jiang Zhang, Bo Zhang, and Qun Jin
\newblock 2021.
\newblock {\em Research and Implementation of Chinese Couplet
Generation System With Attention Based
Transformer Mechanism}
\newblock IEEE transactions on computational social systems. Published online 2021:1-9. doi:10.1109/TCSS.2021.3072153.




\end{thebibliography}

\end{document}