{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "danish-nation",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "realistic-truck",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 26.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 88.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.53.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.1.2 tokenizers-0.10.3 transformers-4.12.5\n",
      "Collecting pypinyin\n",
      "  Downloading pypinyin-0.44.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pypinyin\n",
      "Successfully installed pypinyin-0.44.0\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=149984bd26f95005524899f30f3df42a7eff6e03b32d07c2911817f7e2f19bcf\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Collecting paddlepaddle\n",
      "  Downloading paddlepaddle-2.2.0-cp38-cp38-manylinux1_x86_64.whl (108.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 108.2 MB 90.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 83.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (2.24.0)\n",
      "Collecting astor\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.13; python_version >= \"3.5\" and platform_system != \"Windows\" in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (1.19.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (1.15.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n",
      "Installing collected packages: Pillow, astor, paddlepaddle\n",
      "Successfully installed Pillow-8.4.0 astor-0.8.1 paddlepaddle-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pypinyin\n",
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unusual-large",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "composed-annex",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AnchiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re,time,json\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from pypinyin import pinyin, Style\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import (BertTokenizer,BertConfig,BertModel)\n",
    "\n",
    "from model.Embedding import FusionEmedding,GlyphEmbedding,PinyinEmbedding\n",
    "from model.fusionDataset import FusionDataset\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "\n",
    "config = BertConfig.from_pretrained('AnchiBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('AnchiBERT')\n",
    "with open('AnchiBERT/vocab.txt','r') as f:\n",
    "    ix2BertChar = defaultdict(lambda: '[UNK]') \n",
    "    ix2BertChar.update({ i: _ for (i,_) in enumerate(f.readlines())})\n",
    "Anchibert = BertModel.from_pretrained('AnchiBERT',config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "institutional-passage",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "dated-kenya",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0421, -0.0327,  0.0096,  ...,  0.0711,  0.0417, -0.0224],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2590,  0.2719,  0.1614,  0.0099,  0.1555, -0.2728,  0.2700, -0.1006,\n",
       "         -0.0885,  0.0607,  0.2121,  0.1352,  0.1809, -0.0842,  0.2739, -0.2115,\n",
       "         -0.2756, -0.1202,  0.0751, -0.2282, -0.2437, -0.2295,  0.2278, -0.2172,\n",
       "          0.0318, -0.2727,  0.0463, -0.2532, -0.1979,  0.1673]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glyph = GlyphEmbedding('data/glyph_weight.npy')\n",
    "pinyinV = PinyinEmbedding(30,'data/pinyin_map.json')\n",
    "# Example\n",
    "display(glyph(torch.tensor([0,1,2])))\n",
    "display(pinyinV(torch.tensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optional-rugby",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "convenient-document",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  100, 6789,  100,  689, 2591,  223,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "_ = tokenizer('[UNK]辛[UNK]业怠π',return_tensors='pt')\n",
    "display(_)\n",
    "Anchibert(**_).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entitled-tractor",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "impossible-japan",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "with open('data/char_map.json','r') as f:\n",
    "    glyph2ix = defaultdict(lambda : 1)\n",
    "    glyph2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    glyph2ix.update(json.load(f))\n",
    "\n",
    "with open('data/pinyin_map.json','r') as f:\n",
    "    pinyin2ix = defaultdict(lambda : 1)\n",
    "    pinyin2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    pinyin2ix.update(json.load(f))\n",
    "    \n",
    "with open('data/pos_tags.json','r') as f:\n",
    "    pos2ix = defaultdict(lambda : 0)\n",
    "    pos2ix.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accurate-trauma",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "finnish-guide",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# train 上联\n",
    "with open(\"couplet/train/in.txt\",encoding='utf8') as f:\n",
    "    tr_in =  [row.strip().split() for row in f.readlines()]\n",
    "# train 下联  \n",
    "with open(\"couplet/train/out.txt\",encoding='utf8') as f:\n",
    "    tr_out = [row.strip().split() for row in f.readlines()]\n",
    "with open('data/train_in_pos.pt','rb') as f:\n",
    "    tr_pos_in = pickle.load(f)\n",
    "with open('data/train_out_pos.pt','rb') as f:\n",
    "    tr_pos_out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "african-catholic",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "variable-arrival",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 2097.24it/s]\n",
      "110it [00:00, 2196.09it/s]\n"
     ]
    }
   ],
   "source": [
    "trainSet = FusionDataset(tr_in[:110],tokenizer,glyph2ix,pinyin2ix,pos2ix,tr_out[:110],tr_pos_in[:110],tr_pos_out[:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(trainSet,batch_size=64,shuffle=True)\n",
    "#     validLoader = DataLoader(validSet,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "inside-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'font_weight_path':'data/glyph_weight.npy',\n",
    "    'pinyin_embed_dim':30,\n",
    "    'pinyin_path':'data/pinyin_map.json',\n",
    "    'tag_size':30,\n",
    "    'tag_emb_dim':10,\n",
    "    'layer_norm_eps':1e-12,\n",
    "    'hidden_dropout':0.1,\n",
    "    'nhead':12,\n",
    "    'num_layers':6,\n",
    "    'output_dim':len(ix2BertChar)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "together-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinyin_Glyph_Pos_Anchi_Transformer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = FusionEmedding(config)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=config['hidden_size'], nhead=config['nhead'])\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=config['num_layers'])\n",
    "        \n",
    "        self.Linear = nn.Linear(config['hidden_size'],config['output_dim'])\n",
    "    def forward(self,Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                Xsents_token_type_ids,Xsents_pos_ids,\\\n",
    "                Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                Ysents_token_type_ids,Ysents_pos_ids):\n",
    "        \n",
    "        memory = self.embedding(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                                Xsents_token_type_ids,Xsents_pos_ids).permute([1,0,2])\n",
    "        tgt = self.embedding(Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                             Ysents_token_type_ids,Ysents_pos_ids).permute([1,0,2])\n",
    "        outputs = self.transformer_decoder(tgt, memory)\n",
    "        outputs = self.Linear(out)\n",
    "        \n",
    "        return F.log_softmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "external-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pinyin_Glyph_Pos_Anchi_Transformer(\n",
       "  (embedding): FusionEmedding(\n",
       "    (position_embeddings): Embedding(50, 768)\n",
       "    (glyph_embeddings): GlyphEmbedding(\n",
       "      (embedding): Embedding(9110, 576)\n",
       "    )\n",
       "    (pinyin_embeddings): PinyinEmbedding(\n",
       "      (embedding): Embedding(1297, 30, padding_idx=0)\n",
       "    )\n",
       "    (pos_tag_embeddings): Embedding(30, 10, padding_idx=0)\n",
       "    (fc): Linear(in_features=1384, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Linear): Linear(in_features=768, out_features=21128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pinyin_Glyph_Pos_Anchi_Transformer(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bigger-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anchibert.to(device)\n",
    "for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "    Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "    Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "    Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "    Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "    Ysents_glyph_ids,Ysents_pos_ids in trainLoader:\n",
    "    \n",
    "    Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "    Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "    Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "    Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "    Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "    Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "    Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "    Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "    Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "    Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "    Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "    Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "    \n",
    "    Xword_embeddings = Anchibert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "    Yword_embeddings = Anchibert(Ysents_input_ids,      \\\n",
    "                                 Ysents_token_type_ids, \\\n",
    "                                 Ysents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    \n",
    "    outputs = model(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                    Xsents_token_type_ids,Xsents_pos_ids,\\\n",
    "                    Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                    Ysents_token_type_ids,Ysents_pos_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tutorial-superintendent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 64, 21128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "appropriate-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionEmedding(config)\n",
    "model.to(device)\n",
    "Anchibert.to(device)\n",
    "for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "    Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "    Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "    Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "    Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "    Ysents_glyph_ids,Ysents_pos_ids in trainLoader:\n",
    "    \n",
    "    Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "    Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "    Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "    Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "    Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "    Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "    Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "    Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "    Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "    Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "    Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "    Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "    \n",
    "    Xword_embeddings = Anchibert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    outputembeddingsX = model(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                             Xsents_token_type_ids,Xsents_pos_ids)\n",
    "\n",
    "    Yword_embeddings = Anchibert(Ysents_input_ids,      \\\n",
    "                                 Ysents_token_type_ids, \\\n",
    "                                 Ysents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    \n",
    "    outputembeddingsY = model(Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                             Ysents_token_type_ids,Ysents_pos_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "detailed-discretion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 29, 768]), torch.Size([64, 28, 768]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputembeddingsX.shape,outputembeddingsY.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
