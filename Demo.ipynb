{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-invite",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "extended-trigger",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pypinyin\n",
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-insertion",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "wrong-airport",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "c:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Some weights of the model checkpoint at AnchiBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re,time,json\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from pypinyin import pinyin, Style\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import (BertTokenizer,BertConfig,BertModel)\n",
    "\n",
    "from model.Embedding import FusionEmbedding,GlyphEmbedding,PinyinEmbedding\n",
    "from model.fusionDataset import FusionDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "\n",
    "config = BertConfig.from_pretrained('AnchiBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('AnchiBERT')\n",
    "Anchibert = BertModel.from_pretrained('AnchiBERT',config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-vacuum",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "massive-ministry",
     "kernelId": ""
    }
   },
   "source": [
    "### Example for glyph + pinyin embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appointed-homeless",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fuzzy-corner",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0720, -0.0596,  0.0230,  ...,  0.0150, -0.0975, -0.0763],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2357, -0.2738, -0.2652,  0.1372, -0.0071, -0.3078,  0.0094,  0.0898,\n",
       "         -0.1926, -0.2237,  0.1600,  0.1161,  0.2236, -0.0088,  0.1961, -0.2132,\n",
       "          0.2560,  0.2003, -0.0597,  0.0148,  0.1607,  0.1465,  0.0915, -0.0037,\n",
       "         -0.2926,  0.2344,  0.2502,  0.0166, -0.0071,  0.0503],\n",
       "        [-0.1495, -0.2923, -0.2148, -0.2737, -0.1621,  0.2301,  0.2625,  0.1725,\n",
       "         -0.2636, -0.0225,  0.2649,  0.3154, -0.1482,  0.1330,  0.1423,  0.1166,\n",
       "          0.2852,  0.1410,  0.1775, -0.1640,  0.0467, -0.0750,  0.2550, -0.0938,\n",
       "          0.1038,  0.1584,  0.1917,  0.0675, -0.2958, -0.1075]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glyph = GlyphEmbedding('data/glyph_weight.npy')\n",
    "pinyinV = PinyinEmbedding(30,'data/pinyin_map.json')\n",
    "# Example\n",
    "display(glyph(torch.tensor([0,1,2])))\n",
    "display(pinyinV(torch.tensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pharmaceutical-usage",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "equal-store",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  100, 6789,  689, 2591,  223,    0,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "_ = tokenizer('[UNK]辛业怠π[PAD]',return_tensors='pt')\n",
    "display(_)\n",
    "Anchibert(**_).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solid-spelling",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "gorgeous-perception",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '辛', '业', '怠', 'π', '[PAD]', '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(_['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-microwave",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "front-blogger",
     "kernelId": ""
    }
   },
   "source": [
    "### Load Necessary preproceeded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "egyptian-residence",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "funky-penguin",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('data/char_map.json','r') as f:\n",
    "    glyph2ix = defaultdict(lambda : 1)\n",
    "    glyph2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    glyph2ix.update(json.load(f))\n",
    "\n",
    "with open('data/pinyin_map.json','r') as f:\n",
    "    pinyin2ix = defaultdict(lambda : 1)\n",
    "    pinyin2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    pinyin2ix.update(json.load(f))\n",
    "    \n",
    "with open('data/pos_tags.json','r') as f:\n",
    "    pos2ix = defaultdict(lambda : 0)\n",
    "    pos2ix.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "excellent-elite",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "alike-appointment",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# train 上联\n",
    "with open(\"couplet/train/in.txt\",encoding='utf8') as f:\n",
    "    tr_in =  [row.strip().split() for row in f.readlines()]\n",
    "# train 下联  \n",
    "with open(\"couplet/train/out.txt\",encoding='utf8') as f:\n",
    "    tr_out = [row.strip().split() for row in f.readlines()]\n",
    "with open('data/train_in_pos.pt','rb') as f:\n",
    "    tr_pos_in = pickle.load(f)\n",
    "with open('data/train_out_pos.pt','rb') as f:\n",
    "    tr_pos_out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "improving-fundamental",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "temporal-replacement",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693441it [04:58, 2326.35it/s]\n",
      "693441it [05:03, 2284.49it/s]\n",
      "77050it [00:34, 2202.34it/s]\n",
      "77050it [00:33, 2307.76it/s]\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(tr_in) * 0.9)\n",
    "trainSet = FusionDataset(tr_in[:split_idx],tokenizer,glyph2ix,pinyin2ix,pos2ix,tr_out[:split_idx],tr_pos_in[:split_idx],tr_pos_out[:split_idx])\n",
    "valSet = FusionDataset(tr_in[split_idx:],tokenizer,glyph2ix,pinyin2ix,pos2ix,tr_out[split_idx:],tr_pos_in[split_idx:],tr_pos_out[split_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "regulation-torture",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "returning-spiritual",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainLoader = DataLoader(trainSet,batch_size=batch_size,shuffle=True)\n",
    "validLoader = DataLoader(valSet,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statutory-indicator",
   "metadata": {
    "gradient": {
     "execution_count": 12,
     "id": "standard-budapest",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from model.fusion_transformer import Fusion_Anchi_Trans_Decoder, Fusion_Anchi_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "swedish-classroom",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "green-college",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "config = { # for Fusion_Anchi_Trans_Decoder\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'font_weight_path':'data/glyph_weight.npy',\n",
    "    'pinyin_embed_dim':30,\n",
    "    'pinyin_path':'data/pinyin_map.json',\n",
    "    'tag_size':30,\n",
    "    'tag_emb_dim':10,\n",
    "    'layer_norm_eps':1e-12,\n",
    "    'hidden_dropout':0.1,\n",
    "    'nhead':12,\n",
    "    'num_layers':6,\n",
    "    'output_dim':21128 # fixed\n",
    "}\n",
    "config2 = { # Fusion_Anchi_Transformer\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'font_weight_path':'data/glyph_weight.npy',\n",
    "    'pinyin_embed_dim':30,\n",
    "    'pinyin_path':'data/pinyin_map.json',\n",
    "    'tag_size':30,\n",
    "    'tag_emb_dim':10,\n",
    "    'layer_norm_eps':1e-12,\n",
    "    'hidden_dropout':0.1,\n",
    "    'nhead':12,\n",
    "    'num_encoder_layers':6,\n",
    "    'num_decoder_layers':6,\n",
    "    'output_dim':21128, # fixed\n",
    "    'dim_feedforward': 3072,\n",
    "    'activation':'relu',\n",
    "    'trans_dropout':0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "future-washer",
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "disturbed-analysis",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "model = Fusion_Anchi_Trans_Decoder(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understanding-musical",
   "metadata": {
    "gradient": {
     "execution_count": 15,
     "id": "trained-demand",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "model2 = Fusion_Anchi_Transformer(config2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-integration",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "execution_count": 16,
     "id": "suspected-consumption",
     "kernelId": ""
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FusionEmbedding(\n",
       "  (position_embeddings): Embedding(50, 768)\n",
       "  (glyph_embeddings): GlyphEmbedding(\n",
       "    (embedding): Embedding(9110, 576)\n",
       "  )\n",
       "  (pinyin_embeddings): PinyinEmbedding(\n",
       "    (embedding): Embedding(1297, 30, padding_idx=0)\n",
       "  )\n",
       "  (pos_tag_embeddings): Embedding(30, 10, padding_idx=0)\n",
       "  (fc): Linear(in_features=1384, out_features=768, bias=True)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model3 = FusionEmbedding(config)\n",
    "# model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93e4163a-8844-402a-9558-a6bff502008e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6c4c148aa990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAnchibert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidLoader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-ee4265469c5a>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(net, Anchibert, data, num_epochs, save_model_name, optimizer, criterion, device, valid_loss_min)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAnchibert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-96e991eed6c2>\u001b[0m in \u001b[0;36mepoch_train\u001b[1;34m(net, Anchibert, dataLoader, optimizer, criterion, device, mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "save_model_name = 'decoder_md.pt'\n",
    "\n",
    "model = Fusion_Anchi_Trans_Decoder(config).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "train_net(model, Anchibert, [trainLoader, validLoader], epochs, save_model_name, optimizer, criterion, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aee7cf8e-6239-48ad-9963-1093943db331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(net, Anchibert, dataLoader, optimizer, criterion, device = 'cuda0', mode = 'train'):\n",
    "    valid_loss = 0.0\n",
    "    for data in dataLoader:\n",
    "        data = [data_sub.to(device) for data_sub in data]\n",
    "        \n",
    "        Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "        Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "        Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "        Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "        Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "        Ysents_glyph_ids,Ysents_pos_ids = data\n",
    "\n",
    "        \n",
    "        Xword_embeddings = Anchibert(Xsents_input_ids,      \\\n",
    "                                     Xsents_token_type_ids, \\\n",
    "                                     Xsents_attention_mask  \\\n",
    "                                    )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "        Yword_embeddings = Anchibert(Ysents_input_ids,      \\\n",
    "                                     Ysents_token_type_ids, \\\n",
    "                                     Ysents_attention_mask  \\\n",
    "                                    )['last_hidden_state'].detach()\n",
    "        \n",
    "        # outputs1 for Fusion_Anchi_Trans_Decoder\n",
    "        outputs1 = model(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                        Xsents_token_type_ids,Xsents_pos_ids,\\\n",
    "                        Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                        Ysents_token_type_ids,Ysents_pos_ids,\\\n",
    "                        Xsents_attention_mask.bool(),Ysents_attention_mask.bool())\n",
    "\n",
    "        # get the true model output label from decoder input id\n",
    "        trueY = torch.zeros(Ysents_input_ids.shape,dtype=torch.long,device=device)\n",
    "        trueY[:,:-1] = Ysents_input_ids[:,1:]\n",
    "        # find the index of first padding return 0 if no_padding \n",
    "        indices = torch.argmin(Ysents_attention_mask,dim=1).tolist()\n",
    "\n",
    "        for i, index in enumerate(indices):\n",
    "            trueY[i,index-1] = 102\n",
    "        trueY = trueY.view(-1)\n",
    "\n",
    "        outputs1 = outputs1.view(-1,outputs1.shape[-1])    \n",
    "        loss = criterion(outputs1,trueY)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            valid_loss += loss.item()*data[0].size(0)\n",
    "            \n",
    "    valid_loss = valid_loss/len(dataLoader.dataset)\n",
    "    return valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04ba5e17-702c-4930-b5e9-036b423adf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, Anchibert, data, num_epochs, save_model_name, optimizer, criterion, device = 'cuda0', valid_loss_min = np.Inf):\n",
    "    train_data = data[0]\n",
    "    valid_data = data[1]\n",
    "    \n",
    "    net = net.to(device)\n",
    "    Anchibert = Anchibert.to(device)\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        net = net.train()\n",
    "        _ = epoch_train(net, Anchibert, train_data, optimizer, criterion, device)\n",
    "\n",
    "        net = net.eval()\n",
    "        valid_loss = epoch_train(net, Anchibert, train_data, optimizer, criterion, device, mode = 'valid')\n",
    "        \n",
    "        \n",
    "        print(f'epoch = {i}, loss = {valid_loss}')\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'epoch = {i}')\n",
    "            print('Validation loss ({:.4f} --> {:.4f}).'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "\n",
    "            torch.save(net.state_dict(), save_model_name)\n",
    "            valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "continued-blowing",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "established-margin",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "Anchibert.to(device)\n",
    "for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "    Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "    Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "    Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "    Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "    Ysents_glyph_ids,Ysents_pos_ids in trainLoader:\n",
    "    \n",
    "    Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "    Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "    Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "    Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "    Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "    Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "    Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "    Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "    Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "    Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "    Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "    Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "    \n",
    "    Xword_embeddings = Anchibert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "    Yword_embeddings = Anchibert(Ysents_input_ids,      \\\n",
    "                                 Ysents_token_type_ids, \\\n",
    "                                 Ysents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    # outputs1 for Fusion_Anchi_Trans_Decoder\n",
    "    outputs1 = model(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                    Xsents_token_type_ids,Xsents_pos_ids,\\\n",
    "                    Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                    Ysents_token_type_ids,Ysents_pos_ids,\\\n",
    "                    Xsents_attention_mask.bool(),Ysents_attention_mask.bool())\n",
    "    \n",
    "    # outputs2 Fusion_Anchi_Transformer(config2)\n",
    "#     outputs2= model2(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "#                     Xsents_token_type_ids,Xsents_pos_ids,\\\n",
    "#                     Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "#                     Ysents_token_type_ids,Ysents_pos_ids,\\\n",
    "#                     Xsents_attention_mask.bool(),Ysents_attention_mask.bool())\n",
    "    \n",
    "    # FusionEmbedding\n",
    "#     outputembeddingsX = model3(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "#                          Xsents_token_type_ids,Xsents_pos_ids)\n",
    "#     outputembeddingsY = model3(Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "#                              Ysents_token_type_ids,Ysents_pos_ids)\n",
    "    \n",
    "    \n",
    "    # get the true model output label from decoder input id\n",
    "    trueY = torch.zeros(Ysents_input_ids.shape,dtype=torch.long,device=device)\n",
    "    trueY[:,:-1] = Ysents_input_ids[:,1:]\n",
    "    # find the index of first padding return 0 if no_padding \n",
    "    indices = torch.argmin(Ysents_attention_mask,dim=1).tolist()\n",
    "\n",
    "    for i, index in enumerate(indices):\n",
    "        trueY[i,index-1] = 102\n",
    "    trueY = trueY.view(-1)\n",
    "    \n",
    "    outputs1 = outputs1.view(-1,outputs1.shape[-1])    \n",
    "    loss = criterion(outputs1,trueY)\n",
    "    loss.backward()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e1e0fe7-a86f-4f55-aed2-8df7947332a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1792])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trueY.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-texas",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "valued-extent",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "outputs.shape, outputs2.shape, outputembeddingsX.shape,outputembeddingsY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-purple",
   "metadata": {},
   "source": [
    "### Way to generate new decoder input from raw sentence \n",
    "\n",
    "记得把model output 中 不存在 glyph dict的词以及'[SEP]' 和'[PAD]'给转换成 '_'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "featured-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['晨', '露', '润', '花', '花', '更', '红'], ['万', '方', '乐', '奏', '有', '于', '阗']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = tr_out[:2]\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cognitive-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG:jieba._compat:Paddle enabled successfully......\n",
      "2it [00:00, 83.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([ 101, 3247, 7463, 3883, 5709, 5709, 3291, 5273], dtype=torch.int32),\n",
       "  tensor([ 101,  674, 3175,  727, 1941, 3300,  754,  100], dtype=torch.int32)],\n",
       " [tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)],\n",
       " [tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)],\n",
       " [tensor([  0, 717,  59, 873, 579, 579, 217, 334], dtype=torch.int32),\n",
       "  tensor([   0,  187,  476,  501,  807, 1178,  263, 1002], dtype=torch.int32)],\n",
       " [tensor([   0, 5656, 1568, 3172, 3915, 3915, 3367, 8111], dtype=torch.int32),\n",
       "  tensor([   0, 1729, 3171, 4526, 2058, 8413, 3025, 6866], dtype=torch.int32)],\n",
       " [tensor([ 0,  4,  4,  4,  4,  4, 16, 12], dtype=torch.int32),\n",
       "  tensor([ 0, 26, 26, 26, 26,  9, 27, 27], dtype=torch.int32)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FusionDataset.prepare_sequence(sents=sents, tokenizer=tokenizer,\n",
    "                               glyph2ix=glyph2ix,pinyin2ix=pinyin2ix,\n",
    "                               pos2ix=pos2ix,encode=False,skip_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-breakdown",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
