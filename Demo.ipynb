{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superb-moral",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "boolean-diving",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install pypinyin\n",
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continuous-gentleman",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "metropolitan-times",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AnchiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re,time,json\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from pypinyin import pinyin, Style\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import (BertTokenizer,BertConfig,BertModel)\n",
    "\n",
    "from model.Embedding import *\n",
    "from model.fusionDataset import FusionDataset\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "\n",
    "config = BertConfig.from_pretrained('AnchiBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('AnchiBERT')\n",
    "Anchibert = BertModel.from_pretrained('AnchiBERT',config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-operations",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "asian-sustainability",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "source": [
    "### Example for glyph + pinyin embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "martial-belly",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "sporting-netscape",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0421, -0.0327,  0.0096,  ...,  0.0711,  0.0417, -0.0224],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2629,  0.0584, -0.0224, -0.1282,  0.2653,  0.2522,  0.0376,  0.0921,\n",
       "          0.0478,  0.1703,  0.2385, -0.1221,  0.1052,  0.2103, -0.0876, -0.0335,\n",
       "         -0.2250,  0.2317, -0.2188,  0.2284, -0.0873, -0.0754, -0.0487, -0.1337,\n",
       "          0.0356,  0.0856, -0.1935,  0.0685,  0.2788,  0.2450]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glyph = GlyphEmbedding('data/glyph_weight.npy')\n",
    "pinyinV = PinyinEmbedding(30,'data/pinyin_map.json')\n",
    "# Example\n",
    "display(glyph(torch.tensor([0,1,2])))\n",
    "display(pinyinV(torch.tensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-twenty",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "integrated-lodging",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  100, 6789,  689, 2591,  223,    0,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "_ = tokenizer('[UNK]辛业怠π[PAD]',return_tensors='pt')\n",
    "display(_)\n",
    "Anchibert(**_).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-washer",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "optimum-category",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '辛', '业', '怠', 'π', '[PAD]', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(_['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-metro",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "stable-checkout",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "source": [
    "### Load Necessary preproceeded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "descending-genre",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "funky-measurement",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "with open('data/char_map.json','r') as f:\n",
    "    glyph2ix = defaultdict(lambda : 1)\n",
    "    glyph2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    glyph2ix.update(json.load(f))\n",
    "\n",
    "with open('data/pinyin_map.json','r') as f:\n",
    "    pinyin2ix = defaultdict(lambda : 1)\n",
    "    pinyin2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    pinyin2ix.update(json.load(f))\n",
    "    \n",
    "with open('data/pos_tags.json','r') as f:\n",
    "    pos2ix = defaultdict(lambda : 0)\n",
    "    pos2ix.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-ethiopia",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "whole-israeli",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "# train 上联\n",
    "with open(\"couplet/train/in.txt\",encoding='utf8') as f:\n",
    "    tr_in =  [row.strip().split() for row in f.readlines()]\n",
    "# train 下联  \n",
    "with open(\"couplet/train/out.txt\",encoding='utf8') as f:\n",
    "    tr_out = [row.strip().split() for row in f.readlines()]\n",
    "with open('data/train_in_pos.pt','rb') as f:\n",
    "    tr_pos_in = pickle.load(f)\n",
    "with open('data/train_out_pos.pt','rb') as f:\n",
    "    tr_pos_out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-registrar",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "designing-stage",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770491"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "77049"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(tr_in))\n",
    "total_len = len(tr_in)\n",
    "half = total_len//10\n",
    "display(half)\n",
    "train_split = int(0.95 * half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hispanic-dressing",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "played-lancaster",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73196it [00:32, 2282.21it/s]\n",
      "73196it [00:33, 2200.30it/s]\n"
     ]
    }
   ],
   "source": [
    "trainSet = FusionDataset(tr_in[:train_split],tokenizer,\\\n",
    "                         glyph2ix,pinyin2ix,pos2ix,tr_out[:train_split],\\\n",
    "                         tr_pos_in[:train_split],tr_pos_out[:train_split],\\\n",
    "                         device=device) # use device if you want to load it gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-potential",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 11,
     "id": "ordered-significance",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3853it [00:01, 2360.76it/s]\n",
      "3853it [00:01, 2300.90it/s]\n"
     ]
    }
   ],
   "source": [
    "valSet = FusionDataset(tr_in[train_split:half],tokenizer,\\\n",
    "                       glyph2ix,pinyin2ix,pos2ix,tr_out[train_split:half],\\\n",
    "                       tr_pos_in[train_split:half],tr_pos_out[train_split:half],\\\n",
    "                       device=device) # use device if you want to load it gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-corruption",
   "metadata": {
    "gradient": {
     "id": "corporate-trader",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mental-bookmark",
   "metadata": {
    "gradient": {
     "execution_count": 5,
     "id": "dedicated-starter",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6e80dd330>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IF_WRITER = False\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('pytorch_results')\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "protecting-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-persian",
   "metadata": {},
   "source": [
    "#### Check Time Cost for different Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "instant-angle",
   "metadata": {
    "gradient": {
     "execution_count": 32,
     "id": "sitting-consortium",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model,dataLoader,optimizer,loss_function,\n",
    "                bert, device,with_trans=True):\n",
    "    \"\"\"Train one epoch of the model\"\"\"\n",
    "    epoch_loss = 0    \n",
    "    model.train()\n",
    "    \n",
    "    embed_time = 0\n",
    "    forward_time = 0\n",
    "    cal_y_time = 0\n",
    "    loss_opti_time = 0\n",
    "    \n",
    "    for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "        Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "        Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "        Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "        Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "        Ysents_glyph_ids,Ysents_pos_ids,trueY in tqdm(dataLoader):\n",
    "#         Ysents_glyph_ids,Ysents_pos_ids,trueY in dataLoader:\n",
    "\n",
    "#         Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "#         Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "#         Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "#         Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "#         Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "#         Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "#         Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "#         Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "#         Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "#         Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "#         Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "#         Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "        \n",
    "        s1 = time.time()\n",
    "        Xword_embeddings = bert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                 )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "        Yword_embeddings = bert(Ysents_input_ids,      \\\n",
    "                                Ysents_token_type_ids, \\\n",
    "                                Ysents_attention_mask  \\\n",
    "                               )['last_hidden_state'].detach()\n",
    "        s2 = time.time()\n",
    "        embed_time += (s2-s1)\n",
    "        inputs = {'Xword_embeddings':Xword_embeddings, \\\n",
    "                  'Xsents_pinyin_ids':Xsents_pinyin_ids, \\\n",
    "                  'Xsents_glyph_ids':Xsents_glyph_ids,\\\n",
    "                  'Xsents_pos_ids':Xsents_pos_ids,\\\n",
    "                  'Yword_embeddings':Yword_embeddings,\\\n",
    "                  'Ysents_pinyin_ids':Ysents_pinyin_ids, \\\n",
    "                  'Ysents_glyph_ids':Ysents_glyph_ids,\\\n",
    "                  'Ysents_pos_ids':Ysents_pos_ids, \\\n",
    "                  'device':device}\n",
    "        \n",
    "        if with_trans:\n",
    "            inputs['Xpad_hidden_mask'] = (~ Xsents_attention_mask.bool()).detach()\n",
    "            inputs['Ypad_hidden_mask'] = (~ Ysents_attention_mask.bool()).detach()\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**inputs) # [batch_size,max_length,output_dim]\n",
    "        outputs = outputs.view(-1,outputs.shape[-1])   #[batch_size*max_length,output_dim]\n",
    "        \n",
    "        s3 = time.time()\n",
    "        forward_time +=(s3-s2)\n",
    "        \n",
    "#         # get the true model output label from decoder input id\n",
    "#         trueY = torch.zeros(Ysents_input_ids.shape,dtype=torch.long,device=device)\n",
    "#         trueY[:,:-1] = Ysents_input_ids[:,1:]\n",
    "#         # find the index of first padding return 0 if no_padding \n",
    "#         indices = torch.argmin(Ysents_attention_mask,dim=1).tolist()\n",
    "        \n",
    "#         for i, index in enumerate(indices):\n",
    "#             trueY[i,index-1] = 102\n",
    "        \n",
    "        trueY = trueY.view(-1)\n",
    "        \n",
    "        s4 = time.time()\n",
    "        cal_y_time += s4-s3\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_function(outputs,trueY)\n",
    "        \n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        \n",
    "        # Get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        loss_opti_time += time.time()-s4\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataLoader), embed_time,forward_time,cal_y_time,loss_opti_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "plain-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1355/1355 [10:40<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "trainLoader = DataLoader(trainSet,batch_size=128,shuffle=True)\n",
    "model.to(device)\n",
    "Anchibert.to(device)\n",
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "lr = 0.1\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "res = train_epoch(model,trainLoader,optimizer, criterion,Anchibert,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "animated-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.950739338653115,\n",
       " 42.94941735267639,\n",
       " 258.7296085357666,\n",
       " 0.03971695899963379,\n",
       " 316.05510091781616)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-sixth",
   "metadata": {},
   "source": [
    "### Regular Training Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civilian-token",
   "metadata": {
    "gradient": {
     "execution_count": 32,
     "id": "sitting-consortium",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model,dataLoader,optimizer,loss_function,\n",
    "                bert, device,with_trans=True):\n",
    "    \"\"\"Train one epoch of the model\"\"\"\n",
    "    epoch_loss = 0    \n",
    "    model.train()\n",
    "    \n",
    "    for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "        Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "        Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "        Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "        Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "        Ysents_glyph_ids,Ysents_pos_ids,trueY in dataLoader:\n",
    "#         Ysents_glyph_ids,Ysents_pos_ids in tqdm(dataLoader):\n",
    "\n",
    "        Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "        Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "        Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "        Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "        Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "        Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "        Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "        Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "        Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "        Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "        Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "        Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "        \n",
    "        Xword_embeddings = bert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                 )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "        Yword_embeddings = bert(Ysents_input_ids,      \\\n",
    "                                Ysents_token_type_ids, \\\n",
    "                                Ysents_attention_mask  \\\n",
    "                               )['last_hidden_state'].detach()\n",
    "        \n",
    "        inputs = {'Xword_embeddings':Xword_embeddings, \\\n",
    "                  'Xsents_pinyin_ids':Xsents_pinyin_ids, \\\n",
    "                  'Xsents_glyph_ids':Xsents_glyph_ids,\\\n",
    "                  'Xsents_pos_ids':Xsents_pos_ids,\\\n",
    "                  'Yword_embeddings':Yword_embeddings,\\\n",
    "                  'Ysents_pinyin_ids':Ysents_pinyin_ids, \\\n",
    "                  'Ysents_glyph_ids':Ysents_glyph_ids,\\\n",
    "                  'Ysents_pos_ids':Ysents_pos_ids, \\\n",
    "                  'device':device}\n",
    "        \n",
    "        if with_trans:\n",
    "            inputs['Xpad_hidden_mask'] = (~ Xsents_attention_mask.bool()).detach()\n",
    "            inputs['Ypad_hidden_mask'] = (~ Ysents_attention_mask.bool()).detach()\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        outputs = model(**inputs) # [batch_size,max_length,output_dim]\n",
    "        outputs = outputs.view(-1,outputs.shape[-1])   #[batch_size*max_length,output_dim]\n",
    "                \n",
    "        trueY = trueY.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_function(outputs,trueY)\n",
    "        \n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        \n",
    "        # Get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataLoader)\n",
    "\n",
    "def evaluate_epoch(model,dataLoader,loss_function,\n",
    "                   bert, device,with_trans=True):\n",
    "    \"\"\"Evaluate one epoch of the model\"\"\"\n",
    "    epoch_loss = 0    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "         for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "            Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "            Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "            Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "            Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "            Ysents_glyph_ids,Ysents_pos_ids,trueY in dataLoader:\n",
    "#             Ysents_glyph_ids,Ysents_pos_ids in tqdm(dataLoader):\n",
    "\n",
    "#             Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "#             Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "#             Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "#             Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "#             Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "#             Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "#             Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "#             Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "#             Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "#             Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "#             Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "#             Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "\n",
    "            Xword_embeddings = bert(Xsents_input_ids,      \\\n",
    "                                     Xsents_token_type_ids, \\\n",
    "                                     Xsents_attention_mask  \\\n",
    "                                     )['last_hidden_state'].detach()\n",
    "\n",
    "\n",
    "            Yword_embeddings = bert(Ysents_input_ids,      \\\n",
    "                                    Ysents_token_type_ids, \\\n",
    "                                    Ysents_attention_mask  \\\n",
    "                                   )['last_hidden_state'].detach()\n",
    "\n",
    "            inputs = {'Xword_embeddings':Xword_embeddings, \\\n",
    "                      'Xsents_pinyin_ids':Xsents_pinyin_ids, \\\n",
    "                      'Xsents_glyph_ids':Xsents_glyph_ids,\\\n",
    "                      'Xsents_pos_ids':Xsents_pos_ids,\\\n",
    "                      'Yword_embeddings':Yword_embeddings,\\\n",
    "                      'Ysents_pinyin_ids':Ysents_pinyin_ids, \\\n",
    "                      'Ysents_glyph_ids':Ysents_glyph_ids,\\\n",
    "                      'Ysents_pos_ids':Ysents_pos_ids, \\\n",
    "                      'device':device}\n",
    "\n",
    "            if with_trans:\n",
    "                inputs['Xpad_hidden_mask'] = (~ Xsents_attention_mask.bool()).detach()\n",
    "                inputs['Ypad_hidden_mask'] = (~ Ysents_attention_mask.bool()).detach()\n",
    "\n",
    "\n",
    "            outputs = model(**inputs) # [batch_size,max_length,output_dim]\n",
    "            outputs = outputs.view(-1,outputs.shape[-1])   #[batch_size*max_length,output_dim]\n",
    "\n",
    "            trueY = trueY.view(-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(outputs,trueY)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "established-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeparser(elapse):\n",
    "    mins = int(elapse/60)\n",
    "    sec = int(elapse - (mins*60))\n",
    "    return mins, sec\n",
    "\n",
    "def train(model,trainSet,validSet,batch_size,lr, epoch,bert,name= None,\n",
    "          with_trans= True,if_writer=False,optimizer_name= 'Adam',\n",
    "          scheduleFactor=0.5,schedule_Patience=5,\n",
    "          min_lr=1e-06, verbose=True, patience= 10):\n",
    "    \"\"\"\n",
    "    Training a given neural network model\n",
    "    Return a Best model object in state_dict() fashion\n",
    "    \n",
    "    @Para:\n",
    "    model: pytorch training model\n",
    "    trainSet: Tensor Dataset\n",
    "    validSet: Tensor Dataset\n",
    "    batch_size: batch_size\n",
    "    lr: initial learning rate\n",
    "    epoch: num of epoch\n",
    "    bert: transformers.models.bert.modeling_bert.BertModel\n",
    "    name: Model name\n",
    "    with_trans: If True, a transformer padding mask will be generated as forward input. \n",
    "    \n",
    "    if_writer: True for using tensorboard to trail\n",
    "        the train loss and the valid loss at each epoch\n",
    "    \n",
    "    scheduleFactor: reduce factor for learning rate\n",
    "    schedule_Patience:(int): Number of epochs with no improvement after\n",
    "        which learning rate will be reduced. For example, if\n",
    "        `patience = 2`, then we will ignore the first 2 epochs\n",
    "        with no improvement, and will only decrease the LR after the\n",
    "        3rd epoch if the loss still hasn't improved then.\n",
    "        Default: 10.\n",
    "    \n",
    "    min_lr: min_lr\n",
    "    verbose: True for print train loss and valid loss at each epoch\n",
    "    patience:  Number of epochs with no improvement after\n",
    "        which training will be stopped. For example, if\n",
    "        `patience = 2`, then we will ignore the first 2 epochs\n",
    "        with no improvement, and will only stops after the\n",
    "        3rd epoch if the valid loss still hasn't improved then.\n",
    "        Default: 5.\n",
    "    \"\"\"\n",
    "     # Instantiate Train Loader and Valid Loader\n",
    "    trainLoader = DataLoader(trainSet,batch_size=batch_size,shuffle=True)\n",
    "    validLoader = DataLoader(validSet,batch_size=batch_size,shuffle=False)\n",
    "    if if_writer:\n",
    "        writer = SummaryWriter('pytorch_results')\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    bert.to(device)\n",
    "\n",
    "    # Instantiate loss class\n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "    \n",
    "    # Iniantiate optimizer class\n",
    "    lr = lr\n",
    "    \n",
    "    # Iniantiate optimizer class\n",
    "    if optimizer_name:\n",
    "        optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(),lr = lr, momentum=.9, nesterov=True)\n",
    "\n",
    "     # Reduce on Loss Plateau Learning rate scheduler\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer=optimizer,factor=scheduleFactor,patience=schedule_Patience, min_lr=min_lr,verbose=verbose)\n",
    "    \n",
    "    # Instantiate Best model and Best Valid Loss\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    valid_patience_counter = 0\n",
    "    \n",
    "    n_epoch = epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train model (average)\n",
    "        train_loss = train_epoch(model,trainLoader,optimizer,\n",
    "                                 criterion,bert,device,with_trans)\n",
    "        \n",
    "        # Decay Learning Rate\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "        ##############\n",
    "        # validation #\n",
    "        ##############\n",
    "        valid_loss = evaluate_epoch(model,validLoader,criterion,\n",
    "                                               bert,device,with_trans)\n",
    "        mins,secs = timeparser(time.time()-start_time)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        if if_writer:\n",
    "            writer.add_scalar(f\"{name} Train Loss\", train_loss, epoch)\n",
    "            writer.add_scalar(f\"{name} Valid Loss\", valid_loss,epoch)\n",
    "            writer.add_scalars(f\"{name} Loss/Epoch\", {'train': train_loss,\n",
    "                                                    'valid': valid_loss},\n",
    "                                epoch+1)\n",
    "        \n",
    "        if verbose:\n",
    "            # Print Learning Rate and Loss for ecach Loss\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {mins}m {secs}s')\n",
    "            print(f'\\tTraining Loss: {train_loss:.5f} \\tValidation Loss: {valid_loss:.5f}')\n",
    "        \n",
    "        # Compare the Valid Loss with Best Valid Loss\n",
    "        if valid_loss< best_valid_loss:\n",
    "            # reset counter\n",
    "            valid_patience_counter = 0\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "            best_valid_loss = valid_loss\n",
    "        else:\n",
    "            valid_patience_counter += 1\n",
    "            if valid_patience_counter == patience:\n",
    "                if if_writer:\n",
    "                    writer.flush()\n",
    "                    writer.close()\n",
    "                return best_model,train_losses,valid_losses\n",
    "    if if_writer:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    return best_model, train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-stockholm",
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "protective-sixth",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "from model.fusion_transformer import Fusion_Anchi_Trans_Decoder, Fusion_Anchi_Transformer, Anchi_Decoder,Anchi_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "raised-model",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "rental-settlement",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "config = { # for Fusion_Anchi_Trans_Decoder\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'font_weight_path':'data/glyph_weight.npy',\n",
    "    'pinyin_embed_dim':30,\n",
    "    'pinyin_path':'data/pinyin_map.json',\n",
    "    'tag_size':30,\n",
    "    'tag_emb_dim':10,\n",
    "    'layer_norm_eps':1e-12,\n",
    "    'hidden_dropout':0.1,\n",
    "    'nhead':12,\n",
    "    'num_layers':6,\n",
    "    'output_dim':21128 # fixed\n",
    "}\n",
    "config2 = { # Fusion_Anchi_Transformer\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'font_weight_path':'data/glyph_weight.npy',\n",
    "    'pinyin_embed_dim':30,\n",
    "    'pinyin_path':'data/pinyin_map.json',\n",
    "    'tag_size':30,\n",
    "    'tag_emb_dim':10,\n",
    "    'layer_norm_eps':1e-12,\n",
    "    'hidden_dropout':0.1,\n",
    "    'nhead':12,\n",
    "    'num_encoder_layers':6,\n",
    "    'num_decoder_layers':6,\n",
    "    'output_dim':21128, # fixed\n",
    "    'dim_feedforward': 3072,\n",
    "    'activation':'relu',\n",
    "    'trans_dropout':0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coordinated-pioneer",
   "metadata": {
    "gradient": {
     "execution_count": 16,
     "id": "handmade-ghana",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "model = Fusion_Anchi_Trans_Decoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-channels",
   "metadata": {
    "gradient": {
     "execution_count": 8,
     "id": "suspected-transition",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "# model2 = Fusion_Anchi_Transformer(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-scratch",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "execution_count": 16,
     "id": "fiscal-radius",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FusionEmbedding(\n",
       "  (position_embeddings): Embedding(50, 768)\n",
       "  (glyph_embeddings): GlyphEmbedding(\n",
       "    (embedding): Embedding(9110, 576)\n",
       "  )\n",
       "  (pinyin_embeddings): PinyinEmbedding(\n",
       "    (embedding): Embedding(1297, 30, padding_idx=0)\n",
       "  )\n",
       "  (pos_tag_embeddings): Embedding(30, 10, padding_idx=0)\n",
       "  (fc): Linear(in_features=1384, out_features=768, bias=True)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model3 = FusionEmbedding(config)\n",
    "# model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-greek",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "name = 'fusion_decoder_64_1_60_'\n",
    "best_model,train_losses,valid_losses = train(model,trainSet,valSet,\n",
    "                                          batch_size=64,lr=0.1,\n",
    "                                          epoch=60,bert=Anchibert,\n",
    "                                          name= name,\n",
    "                                          with_trans=True,if_writer=False,\n",
    "                                          optimizer_name='Adam',scheduleFactor=0.5,\n",
    "                                          schedule_Patience=5,min_lr=1e-06,\n",
    "                                          verbose=True,patience=10\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,f'{name}.pt')\n",
    "with open(f'{name}_losses.pt','w') as f:\n",
    "    pickle.dump((train_losses,valid_losses),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-wagon",
   "metadata": {},
   "source": [
    "### Way to generate new decoder input from raw sentence \n",
    "\n",
    "记得把model output 中 不存在 glyph dict的词以及'[SEP]' 和'[PAD]'给转换成 '_'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-rebate",
   "metadata": {
    "gradient": {
     "execution_count": 21,
     "id": "cardiovascular-redhead",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['晨', '露', '润', '花', '花', '更', '红'], ['万', '方', '乐', '奏', '有', '于', '阗']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = tr_out[:2]\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-figure",
   "metadata": {
    "gradient": {
     "execution_count": 24,
     "id": "basic-station",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG:jieba._compat:Paddle enabled successfully......\n",
      "2it [00:00, 83.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([ 101, 3247, 7463, 3883, 5709, 5709, 3291, 5273], dtype=torch.int32),\n",
       "  tensor([ 101,  674, 3175,  727, 1941, 3300,  754,  100], dtype=torch.int32)],\n",
       " [tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)],\n",
       " [tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)],\n",
       " [tensor([  0, 717,  59, 873, 579, 579, 217, 334], dtype=torch.int32),\n",
       "  tensor([   0,  187,  476,  501,  807, 1178,  263, 1002], dtype=torch.int32)],\n",
       " [tensor([   0, 5656, 1568, 3172, 3915, 3915, 3367, 8111], dtype=torch.int32),\n",
       "  tensor([   0, 1729, 3171, 4526, 2058, 8413, 3025, 6866], dtype=torch.int32)],\n",
       " [tensor([ 0,  4,  4,  4,  4,  4, 16, 12], dtype=torch.int32),\n",
       "  tensor([ 0, 26, 26, 26, 26,  9, 27, 27], dtype=torch.int32)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FusionDataset.prepare_sequence(sents=sents, tokenizer=tokenizer,\n",
    "                               glyph2ix=glyph2ix,pinyin2ix=pinyin2ix,\n",
    "                               pos2ix=pos2ix,encode=False,skip_error=False,\n",
    "                               device=device) # use device if you want to load it gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-shore",
   "metadata": {},
   "source": [
    "#### Section for testing train and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thick-reply",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "opening-great",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 2223.45it/s]\n",
      "1000it [00:00, 2323.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tempSet = FusionDataset(tr_in[:1000],tokenizer,glyph2ix,pinyin2ix,pos2ix,tr_out[:1000],tr_pos_in[:1000],tr_pos_out[:1000])\n",
    "tempLoader = DataLoader(tempSet,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "express-christian",
   "metadata": {
    "gradient": {
     "execution_count": 11,
     "id": "rapid-consumer",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.1)\n",
    "criterion = nn.NLLLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "Anchibert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surprised-emphasis",
   "metadata": {
    "gradient": {
     "execution_count": 20,
     "id": "adaptive-guard",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "model = Anchi_Transformer(config2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "noted-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Anchi_Decoder(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "incident-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "model = Fusion_Anchi_Transformer(config2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "subtle-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "model = Fusion_Anchi_Trans_Decoder(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hidden-estimate",
   "metadata": {
    "gradient": {
     "execution_count": 33,
     "id": "mental-smith",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "res = train_epoch(model,tempLoader,optimizer,criterion,Anchibert,device,with_trans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "vietnamese-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_epoch(model,tempLoader,criterion,Anchibert,device,with_trans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-duration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
