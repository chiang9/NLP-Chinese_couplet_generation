{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124ba2ad",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "boolean-diving",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install pypinyin\n",
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60e0b4",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "metropolitan-times",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "import re,time,json,pickle\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import (BertTokenizer,BertConfig,BertModel)\n",
    "\n",
    "from model.fusionDataset import FusionDataset\n",
    "\n",
    "config = BertConfig.from_pretrained('AnchiBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('AnchiBERT')\n",
    "Anchibert = BertModel.from_pretrained('AnchiBERT',config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799130d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/anchi_tra_Adam_128_00001_60_6_6_110k_losses.pt','rb') as f:\n",
    "    losses = pickle.load(f)\n",
    "zoom = 1\n",
    "x = [_ for _ in range(zoom,len(losses[1])+1)]\n",
    "\n",
    "plt.plot(x,losses[0][zoom-1:],label='Train loss')\n",
    "plt.plot(x,losses[1][zoom-1:],label='valid loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0d3e3",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "stable-checkout",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "source": [
    "### Load Necessary preproceeded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e0f1c",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "funky-measurement",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/char_map.json','r') as f:\n",
    "    ix2glyph = defaultdict(lambda : '_')\n",
    "    ix2glyph[0] = '[PAD]'\n",
    "    glyph2ix = defaultdict(lambda : 1)\n",
    "    glyph2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    for i, k in enumerate(json.load(f).keys(),2):\n",
    "        glyph2ix[k] = i\n",
    "        ix2glyph[i] = k\n",
    "with open('data/pinyin_map.json','r') as f:\n",
    "    pinyin2ix = defaultdict(lambda : 1)\n",
    "    pinyin2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    for i,k in enumerate(json.load(f).keys(),2):\n",
    "        pinyin2ix[k] = i\n",
    "with open('data/pos_tags.json','r') as f:\n",
    "    pos2ix = defaultdict(lambda : 0)\n",
    "    pos2ix.update(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc98df",
   "metadata": {},
   "source": [
    "# Decoder Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e314c4f",
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "protective-sixth",
     "kernelId": "583276d5-8a0a-4f47-9ffe-672fe9cb301f"
    }
   },
   "outputs": [],
   "source": [
    "from model.fusion_transformer import Fusion_Anchi_Trans_Decoder, Fusion_Anchi_Transformer, Anchi_Decoder,Anchi_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"couplet/test/in.txt\",encoding='utf8') as f:\n",
    "    te_in =  [row.strip().split() for row in f.readlines()]\n",
    "# train 下联  \n",
    "with open(\"couplet/test/out.txt\",encoding='utf8') as f:\n",
    "    te_out = [row.strip().split() for row in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8278f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { # Anchi_Transformer\n",
    "    'max_position_embeddings':50,\n",
    "    'hidden_size':768,\n",
    "    'layer_norm_eps':1e-12, \n",
    "    'hidden_dropout':0.1, \n",
    "    'nhead':12,\n",
    "    'num_encoder_layers':6, # trainable\n",
    "    'num_decoder_layers':6, # trainable\n",
    "    'output_dim':9110,# fixed use glyph dim as output\n",
    "    'dim_feedforward': 3072,\n",
    "    'activation':'relu',\n",
    "    'trans_dropout':0.1,\n",
    "    'device':device\n",
    "}\n",
    "# <model_name>_<optim>_<batch_num>_<lr>_<epoch>_<encoder layer>_<decoder layer>_<train_data_size>\n",
    "# name = 'anchi_tra_Adam_128_0001_60_6_6_110k'\n",
    "model= Anchi_Transformer(config)\n",
    "model.load_state_dict(torch.load('result/anchi_tra_Adam_128_00001_60_6_6_110k.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_couplet import greedy_decode,beam_search_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = greedy_decode(model=model,\n",
    "                      bert=Anchibert,\n",
    "                      tokenizer=tokenizer,\n",
    "                      sent=te_in[0],\n",
    "                      glyph2ix=glyph2ix,\n",
    "                      pinyin2ix=pinyin2ix,\n",
    "                      pos2ix=pos2ix,\n",
    "                      ix2glyph=ix2glyph,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b33540",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(te_in[0]),' '.join(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e775ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = beam_search_decode(model=model,\n",
    "                            k=5,\n",
    "                          bert=Anchibert,\n",
    "                          tokenizer=tokenizer,\n",
    "                          sent=te_in[0],\n",
    "                          glyph2ix=glyph2ix,\n",
    "                          pinyin2ix=pinyin2ix,\n",
    "                          pos2ix=pos2ix,\n",
    "                          ix2glyph=ix2glyph,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93e29aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彴 镪 云 年 秬 吣 秬 斵 侓 趿 閱 声 焐 艋 何 褵 蜨 瘽 tensor(206.5308, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "彴 镪 云 年 秬 吣 秬 斵 侓 趿 閱 声 焐 艋 何 褵 蜨 梦 tensor(206.5351, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "彴 镪 云 年 秬 吣 秬 斵 侓 趿 閱 声 焐 艋 何 褵 崺 瘽 tensor(206.5449, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "彴 镪 云 年 秬 吣 秬 斵 侓 趿 閱 声 焐 艋 何 褵 崺 梦 tensor(206.5492, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "彴 镪 云 年 秬 吣 秬 斵 侓 趿 閱 声 焐 艋 何 褵 蜨 黒 tensor(206.5609, device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, j in predict:\n",
    "    print( ' '.join(i),j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
