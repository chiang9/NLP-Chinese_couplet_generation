{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "armed-blake",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "realistic-truck",
     "kernelId": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.53.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: pypinyin in /opt/conda/lib/python3.8/site-packages (0.44.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.8/site-packages (0.42.1)\n",
      "Requirement already satisfied: paddlepaddle in /opt/conda/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (4.4.2)\n",
      "Requirement already satisfied: numpy>=1.13; python_version >= \"3.5\" and platform_system != \"Windows\" in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (1.19.2)\n",
      "Requirement already satisfied: astor in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (8.4.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (3.14.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (1.15.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pypinyin\n",
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "successful-feelings",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "composed-annex",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AnchiBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re,time,json\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from pypinyin import pinyin, Style\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import (BertTokenizer,BertConfig,BertModel)\n",
    "\n",
    "from model.Embedding import FusionEmedding,GlyphEmbedding,PinyinEmbedding\n",
    "from model.fusionDataset import FusionDataset\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "\n",
    "config = BertConfig.from_pretrained('AnchiBERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('AnchiBERT')\n",
    "Anchibert = BertModel.from_pretrained('AnchiBERT',config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honest-nancy",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "dated-kenya",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0421, -0.0327,  0.0096,  ...,  0.0711,  0.0417, -0.0224],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1162, -0.0063, -0.1702, -0.0711,  0.0213,  0.0390, -0.2001, -0.1429,\n",
       "          0.2429,  0.0553, -0.2503, -0.0788, -0.2204, -0.2940,  0.0712, -0.2851,\n",
       "          0.1147, -0.2163,  0.2020,  0.2989,  0.3067,  0.2864, -0.0451,  0.0571,\n",
       "          0.0190,  0.0826, -0.1811,  0.3057, -0.2350, -0.2527]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glyph = GlyphEmbedding('data/glyph_weight.npy')\n",
    "pinyinV = PinyinEmbedding(30,'data/pinyin_map.json')\n",
    "# Example\n",
    "display(glyph(torch.tensor([0,1,2])))\n",
    "display(pinyinV(torch.tensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generic-calculator",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "convenient-document",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  100, 6789,  100,  689, 2591,  223,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "_ = tokenizer('[UNK]辛[UNK]业怠π',return_tensors='pt')\n",
    "display(_)\n",
    "Anchibert(**_).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "underlying-guitar",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "impossible-japan",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('data/char_map.json','r') as f:\n",
    "    glyph2ix = defaultdict(lambda : 1)\n",
    "    glyph2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    glyph2ix.update(json.load(f))\n",
    "\n",
    "with open('data/pinyin_map.json','r') as f:\n",
    "    pinyin2ix = defaultdict(lambda : 1)\n",
    "    pinyin2ix.update({'[CLS]':0,'[SEP]':0,'[PAD]':0})\n",
    "    pinyin2ix.update(json.load(f))\n",
    "    \n",
    "with open('data/pos_tags.json','r') as f:\n",
    "    pos2ix = defaultdict(lambda : 0)\n",
    "    pos2ix.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indirect-intensity",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "finnish-guide",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# train 上联\n",
    "with open(\"couplet/train/in.txt\",encoding='utf8') as f:\n",
    "    tr_in =  [row.strip().split() for row in f.readlines()]\n",
    "# train 下联  \n",
    "with open(\"couplet/train/out.txt\",encoding='utf8') as f:\n",
    "    tr_out = [row.strip().split() for row in f.readlines()]\n",
    "with open('data/train_in_pos.pt','rb') as f:\n",
    "    tr_pos_in = pickle.load(f)\n",
    "with open('data/train_out_pos.pt','rb') as f:\n",
    "    tr_pos_out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "infrared-boundary",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "variable-arrival",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 2065.60it/s]\n",
      "110it [00:00, 2233.97it/s]\n"
     ]
    }
   ],
   "source": [
    "trainSet = FusionDataset(tr_in[:110],tokenizer,glyph2ix,pinyin2ix,pos2ix,tr_out[:110],tr_pos_in[:110],tr_pos_out[:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attended-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(trainSet,batch_size=64,shuffle=True)\n",
    "#     validLoader = DataLoader(validSet,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjustable-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anchibert.to(device)\n",
    "for Xsents_input_ids,Xsents_token_type_ids,\\\n",
    "    Xsents_attention_mask,Xsents_pinyin_ids,\\\n",
    "    Xsents_glyph_ids,Xsents_pos_ids,\\\n",
    "    Ysents_input_ids,Ysents_token_type_ids,\\\n",
    "    Ysents_attention_mask,Ysents_pinyin_ids,\\\n",
    "    Ysents_glyph_ids,Ysents_pos_ids in trainLoader:\n",
    "    \n",
    "    Xsents_input_ids = Xsents_input_ids.to(device)\n",
    "    Xsents_token_type_ids = Xsents_token_type_ids.to(device)\n",
    "    Xsents_attention_mask = Xsents_attention_mask.to(device)\n",
    "    Xsents_pinyin_ids = Xsents_pinyin_ids.to(device)\n",
    "    Xsents_glyph_ids = Xsents_glyph_ids.to(device)\n",
    "    Xsents_pos_ids = Xsents_pos_ids.to(device)\n",
    "    Ysents_input_ids = Ysents_input_ids.to(device)\n",
    "    Ysents_token_type_ids = Ysents_token_type_ids.to(device)\n",
    "    Ysents_attention_mask = Ysents_attention_mask.to(device)\n",
    "    Ysents_pinyin_ids = Ysents_pinyin_ids.to(device)\n",
    "    Ysents_glyph_ids = Ysents_glyph_ids.to(device)\n",
    "    Ysents_pos_ids = Ysents_pos_ids.to(device)\n",
    "    \n",
    "    Xword_embeddings = Anchibert(Xsents_input_ids,      \\\n",
    "                                 Xsents_token_type_ids, \\\n",
    "                                 Xsents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    outputembeddingsX = model(Xword_embeddings,Xsents_pinyin_ids, \\\n",
    "                             Xsents_token_type_ids,Xsents_pos_ids)\n",
    "\n",
    "    Yword_embeddings = Anchibert(Ysents_input_ids,      \\\n",
    "                                 Ysents_token_type_ids, \\\n",
    "                                 Ysents_attention_mask  \\\n",
    "                                )['last_hidden_state'].detach()\n",
    "    \n",
    "    outputembeddingsY = model(Yword_embeddings,Ysents_pinyin_ids, \\\n",
    "                             Ysents_token_type_ids,Ysents_pos_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suburban-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 29, 768]), torch.Size([64, 28, 768]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputembeddingsX.shape,outputembeddingsY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-immunology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
